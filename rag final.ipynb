{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BB2HCeoGYoJP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "THtJICrneXnY"
      },
      "outputs": [],
      "source": [
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCdv_P2R4sjaVy_Cd5fQ3GROjYzZJfD4FI\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fenpiAR8e8Ib",
        "outputId": "b74bcdb6-335f-4ff0-b863-5b0a98bf96bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Layer 1 vector store saved to 'faiss_layer1_db'.\n"
          ]
        }
      ],
      "source": [
        "#do not run again if trained embeddings are downloaded \n",
        "# -------------------- LAYER 1: Compute and Save --------------------\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import PyPDF2\n",
        "\n",
        "# Load embedding model\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Read and extract text from PDF\n",
        "pdf_path = \"7.%20Brochure%20-NMP-merged.pdf\"\n",
        "reader = PyPDF2.PdfReader(pdf_path)\n",
        "all_text = \"\"\n",
        "for page in reader.pages:\n",
        "    all_text += page.extract_text() + \"\\n\"\n",
        "\n",
        "# Split text into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "docs = splitter.create_documents([all_text])\n",
        "\n",
        "# Create and save FAISS vector store\n",
        "db = FAISS.from_documents(docs, embedding)\n",
        "db.save_local(\"faiss_layer1_db\")\n",
        "print(\"‚úÖ Layer 1 vector store saved to 'faiss_layer1_db'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Kartik\\AppData\\Local\\Temp\\ipykernel_23304\\2543305420.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "c:\\Users\\Kartik\\OneDrive\\Desktop\\Bajaj Projct\\bajaj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Kartik\\OneDrive\\Desktop\\Bajaj Projct\\bajaj\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "‚úÖ Layer 1 vector store loaded.\n",
            "‚úÖ Layer 1 Hybrid Retriever initialized.\n"
          ]
        }
      ],
      "source": [
        "# -------------------- LAYER 1: Load and Create Hybrid Retriever --------------------\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "# Load embedding model\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load saved FAISS index (Layer 1)\n",
        "db = FAISS.load_local(\"faiss_layer1_db\", embedding, allow_dangerous_deserialization=True)\n",
        "print(\"‚úÖ Layer 1 vector store loaded.\")\n",
        "\n",
        "# Create FAISS retriever\n",
        "faiss_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\n",
        "    \"k\": 6,            # number of final results\n",
        "    \"fetch_k\": 20,     # candidate pool before reranking\n",
        "    \"lambda_mult\": 0.7 # balance between relevance (1.0) and diversity (0.0)\n",
        "})\n",
        "\n",
        "# Create BM25 retriever using stored documents\n",
        "bm25_retriever = BM25Retriever.from_documents(list(db.docstore._dict.values()))\n",
        "bm25_retriever.k = 6\n",
        "\n",
        "# Create hybrid ensemble retriever combining FAISS + BM25\n",
        "layer1_hybrid_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, faiss_retriever],\n",
        "    weights=[0.5, 0.5]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Layer 1 Hybrid Retriever initialized.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "350sonbRfF_6",
        "outputId": "d7c31614-c469-4eb8-eee2-562df49bd57e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Successfully created the text splitter for 'policy.pdf'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Kartik\\OneDrive\\Desktop\\Bajaj Projct\\bajaj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Successfully created the Layer 2 vector store for 'policy.pdf'.\n"
          ]
        }
      ],
      "source": [
        "# This will be our Layer 2 vector store\n",
        "policy_db = None\n",
        "try:\n",
        "    # Load the new policy document (Now Layer 2)\n",
        "    policy_pdf_path = \"testfile.pdf\"  # Make sure this file is uploaded\n",
        "    policy_reader = PyPDF2.PdfReader(policy_pdf_path)\n",
        "    policy_text = \"\"\n",
        "    for page in policy_reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            policy_text += page_text + \"\\n\"\n",
        "\n",
        "    # Split the new document into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100,separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"])\n",
        "    print(\"‚úÖ Successfully created the text splitter for 'policy.pdf'.\")\n",
        "    policy_docs = text_splitter.create_documents([policy_text])\n",
        "\n",
        "    # Create the FAISS vector store for Layer 2\n",
        "    policy_db = FAISS.from_documents(policy_docs, embedding)\n",
        "    print(\"‚úÖ Successfully created the Layer 2 vector store for 'policy.pdf'.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: 'policy.pdf' not found. Please upload the file and try again.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå An error occurred while processing 'policy.pdf': {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7FujreFffX7",
        "outputId": "ca71ff86-576b-45f3-e844-2177e8849f4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LLM and Layer 2 retriever are initialized.\n"
          ]
        }
      ],
      "source": [
        "if policy_db:\n",
        "    # Initialize the Language Model\n",
        "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"models/gemini-1.5-flash\",  # ‚úÖ use full model path\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    # Create a simple retriever for the new document (Layer 2)\n",
        "    layer2_policy_retriever = policy_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
        "\n",
        "    print(\"‚úÖ LLM and Layer 2 retriever are initialized.\")\n",
        "else:\n",
        "    print(\"Skipping initialization because the 'policy.pdf' vector store was not created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQAW1l3CgPDJ",
        "outputId": "58339691-fa3a-4c7a-fbd2-f1d11a89c760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Query refinement chain created (using Layer 1 context).\n"
          ]
        }
      ],
      "source": [
        "if policy_db:\n",
        "    # The refinement chain now uses context from the main database (Layer 1)\n",
        "    refine_query_template = \"\"\"\n",
        "    Based on the initial context from a general insurance database, refine the original question to be more specific.\n",
        "    This refined question will be used to find precise details in a new, specific policy document.\n",
        "    Only output the new, refined question.\n",
        "\n",
        "    Original Question: {question}\n",
        "\n",
        "    Initial Context from General Database:\n",
        "    ---\n",
        "    {context}\n",
        "    ---\n",
        "\n",
        "    Refined Question for the specific policy document:\n",
        "    \"\"\"\n",
        "    refine_prompt = PromptTemplate.from_template(refine_query_template)\n",
        "\n",
        "    # This function is needed because the hybrid retriever returns a list of docs\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # The chain now uses the Layer 1 hybrid retriever for context\n",
        "    refine_query_chain = (\n",
        "        {\"context\": layer1_hybrid_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | refine_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    print(\"‚úÖ Query refinement chain created (using Layer 1 context).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSZHeFhzgs5l",
        "outputId": "8e6659ca-862a-4266-8d3c-815e8db8cdee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Final answering chain created (using Layer 2 context).\n"
          ]
        }
      ],
      "source": [
        "if policy_db:\n",
        "    final_prompt_template = \"\"\"\n",
        "    You are an expert insurance assistant. Answer the user's question based ONLY on the final context provided from the specific policy document.\n",
        "    Be concise and clear. If the context is insufficient, state that the information is not available in the provided document.\n",
        "\n",
        "    Final Context:\n",
        "    ---\n",
        "    {context}\n",
        "    ---\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    final_prompt = PromptTemplate.from_template(final_prompt_template)\n",
        "\n",
        "    # The main chain now uses the refined query to retrieve from the Layer 2 policy retriever\n",
        "    final_rag_chain = (\n",
        "        {\n",
        "            \"context\": refine_query_chain | layer2_policy_retriever | format_docs,\n",
        "            \"question\": refine_query_chain # Pass the refined question to the final prompt\n",
        "        }\n",
        "        | final_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    print(\"‚úÖ Final answering chain created (using Layer 2 context).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2jSnA3dg0GA",
        "outputId": "70b59851-e630-43bf-e4b5-3279db395b9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- üöÄ Executing Reversed RAG Pipeline ---\n",
            "\n",
            "Original Question: What are the expenses covered under AYUSH treatment in the Arogya Sanjeevani Policy - National?,What is the waiting period for coverage of joint replacement surgery under the Arogya Sanjeevani Policy?,What are the co-payment terms under this policy for insured persons above and below 75 years of age?,4. What documents are required to file a reimbursement claim under the Arogya Sanjeevani Policy?, 5. What are the exclusions applicable under the policy related to cosmetic surgery or weight control treatment?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Kartik\\OneDrive\\Desktop\\Bajaj Projct\\bajaj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "c:\\Users\\Kartik\\OneDrive\\Desktop\\Bajaj Projct\\bajaj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "c:\\Users\\Kartik\\OneDrive\\Desktop\\Bajaj Projct\\bajaj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ‚úÖ Final Answer ---\n",
            "AYUSH treatments (Ayurveda, Unani, Siddha, and Homeopathy) are covered up to the sum insured during each policy year.  Specific coverage limits for each system are not detailed.  The reimbursement process for inpatient AYUSH care is not specified beyond this.\n"
          ]
        }
      ],
      "source": [
        "if policy_db:\n",
        "    # Your initial question goes here\n",
        "    original_question = \"What are the expenses covered under AYUSH treatment in the Arogya Sanjeevani Policy - National?,What is the waiting period for coverage of joint replacement surgery under the Arogya Sanjeevani Policy?,What are the co-payment terms under this policy for insured persons above and below 75 years of age?,4. What documents are required to file a reimbursement claim under the Arogya Sanjeevani Policy?, 5. What are the exclusions applicable under the policy related to cosmetic surgery or weight control treatment?\"\n",
        "\n",
        "\n",
        "    print(\"\\n--- üöÄ Executing Reversed RAG Pipeline ---\")\n",
        "    print(f\"\\nOriginal Question: {original_question}\")\n",
        "\n",
        "    # Invoke the full chain to get the final answer.\n",
        "    final_answer = final_rag_chain.invoke(original_question)\n",
        "\n",
        "    print(\"\\n--- ‚úÖ Final Answer ---\")\n",
        "    print(final_answer)\n",
        "else:\n",
        "    print(\"\\nSkipping RAG execution because 'policy.pdf' could not be processed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "strZEVSTg8S1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bajaj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
